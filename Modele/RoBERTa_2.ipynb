{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence_transformers"
      ],
      "metadata": {
        "id": "-uCe7CmAFxVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "import pandas as pd \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "id": "xD1v8Fk8F0h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CrossEncoder('cross-encoder/nli-roberta-base')\n",
        "scores = model.predict([('A man is eating pizza', 'A man eats something')])\n",
        "\n",
        "#Convert scores to labels\n",
        "label_mapping = ['contradiction', 'entailment', 'neutral']\n",
        "labels = [label_mapping[score_max] for score_max in scores.argmax(axis=1)]\n",
        "labels"
      ],
      "metadata": {
        "id": "uXQnp9tVwhmJ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4zWVwaxv1tT"
      },
      "outputs": [],
      "source": [
        "# downloading the dataset from the url\n",
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "#unzip the files\n",
        "!unzip snli_1.0.zip\n",
        "\n",
        "# importing the dataset into dataframes\n",
        "df_train = pd.read_csv(\"snli_1.0/snli_1.0_train.txt\", sep=\"\\t\")\n",
        "df_dev = pd.read_csv(\"snli_1.0/snli_1.0_dev.txt\", sep=\"\\t\")\n",
        "df_test = pd.read_csv(\"snli_1.0/snli_1.0_test.txt\", sep=\"\\t\")\n",
        "\n",
        "# extracting the required columns form the dataset\n",
        "df_train = df_train[['gold_label','sentence1','sentence2']]\n",
        "df_dev = df_dev[['gold_label','sentence1','sentence2']]\n",
        "df_test = df_test[['gold_label','sentence1','sentence2']]\n",
        "\n",
        "# Analyzing the data\n",
        "df_train.groupby('gold_label').count()\n",
        "\n",
        "# removing the entries from all train, dev and test datasets with label '-'\n",
        "df_train = df_train[df_train['gold_label'] != '-']\n",
        "df_dev = df_dev[df_dev['gold_label'] != '-']\n",
        "df_test = df_test[df_test['gold_label'] != '-']\n",
        "\n",
        "df_test = df_test.head(200)   # dropping the rows from the data with NaN values\n",
        "df_train = df_train.dropna(subset = ['sentence2'])\n",
        "df_train.groupby('gold_label').count() "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "premise  = df_test['sentence1'].values\n",
        "hypothesis = df_test['sentence2'].values\n",
        "\n",
        "model.predict([(premise[0], hypothesis[0])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snMbtpefwE0R",
        "outputId": "a941747a-30d5-4325-82a8-0247c7ac5054"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.41002804, -2.0508602 ,  2.702842  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions pour le jeu de test \n",
        "label_mapping = ['contradiction', 'entailment', 'neutral']\n",
        "listt = []\n",
        "for i in range (len(hypothesis)):\n",
        "  preds = model.predict([(premise[i], hypothesis[i])])\n",
        "  labels = [label_mapping[score_max] for score_max in preds.argmax(axis=1)]\n",
        "  listt.append(\"\".join(labels))"
      ],
      "metadata": {
        "id": "llxrxlrpxEUo"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "Y_test = np_utils.to_categorical(le.fit_transform(df_test[\"gold_label\"].values)).astype(\"int64\")\n",
        "Y_predi = np_utils.to_categorical(le.fit_transform(listt)).astype(\"int64\")"
      ],
      "metadata": {
        "id": "asAYWXoF_XFw"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = (np.argmax(Y_predi, axis=1) == np.argmax(Y_test, axis=1)).sum()/Y_test.shape[0] * 100\n",
        "print(\"Accuracy on test set is: %\"+str(test_acc))"
      ],
      "metadata": {
        "id": "U549sU563VeA",
        "outputId": "1339026c-d9ee-458b-e1f9-fd7ba4e7831b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set is: %90.5\n"
          ]
        }
      ]
    }
  ]
}